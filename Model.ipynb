{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1656ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dense\n",
    "from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50, DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ff8ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'dataset/flickr_8k/images/'\n",
    "TRAIN_DIR = 'dataset/flickr_8k/train.txt'\n",
    "VAL_DIR = 'dataset/flickr_8k/val.txt'\n",
    "TEST_DIR = 'dataset/flickr_8k/test.txt'\n",
    "LABEL_DIR = 'dataset/flickr_8k/token.txt'\n",
    "DESCRIPTION_DIR = 'dataset/flickr_8k/description.txt'\n",
    "\n",
    "VOCAB_SIZE = 1000\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f2abd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 30000, 5000, 5000, 5000, 5000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs = []\n",
    "train_captions = []\n",
    "val_imgs = []\n",
    "val_captions = []\n",
    "test_imgs = []\n",
    "test_captions = []\n",
    "with open(TRAIN_DIR, 'r') as f:\n",
    "    for line in f.read().splitlines():\n",
    "        img, caption = line.split('\\t')\n",
    "        train_imgs.append(IMAGE_DIR + img)\n",
    "        train_captions.append(caption.lower())\n",
    "        \n",
    "with open(VAL_DIR, 'r') as f:\n",
    "    for line in f.read().splitlines():\n",
    "        img, caption = line.split('\\t')\n",
    "        val_imgs.append(IMAGE_DIR + img)\n",
    "        val_captions.append(caption.lower())\n",
    "\n",
    "with open(TEST_DIR, 'r') as f:\n",
    "    for line in f.read().splitlines():\n",
    "        img, caption = line.split('\\t')\n",
    "        test_imgs.append(IMAGE_DIR + img)\n",
    "        test_captions.append(caption.lower())\n",
    "        \n",
    "len(train_imgs), len(train_captions), len(val_imgs), len(val_captions), len(test_imgs), len(test_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee1db687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54825c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([  2,  15,   9,   7,  33, 256,   2,  14,   9,   4], dtype=int64)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_vectorizer(train_sentences):\n",
    "    vectorizer = tf.keras.layers.TextVectorization( \n",
    "        standardize=None,\n",
    "        max_tokens=VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_LENGTH, \n",
    "        ragged=False\n",
    "    ) \n",
    "    \n",
    "    vectorizer.adapt(train_sentences)\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "caption_only = train_dataset.map(lambda img, caption: caption)\n",
    "vectorizer = fit_vectorizer(caption_only)\n",
    "vectorizer(train_captions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "393f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, is_training=True):\n",
    "    dataset = dataset.map(lambda img, caption: (img, vectorizer(caption)))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset_final = preprocess_dataset(train_dataset, is_training=True)\n",
    "val_dataset_final = preprocess_dataset(train_dataset, is_training=True)\n",
    "test_dataset_final = preprocess_dataset(train_dataset, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1a75e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN base model (InceptionV3)\n",
    "# cnn_base_model = InceptionV3(include_top=True, weights='imagenet')\n",
    "# cnn_model = Model(inputs=cnn_base_model.input, outputs=cnn_base_model.layers[-2].output)\n",
    "\n",
    "cnn_base_model = DenseNet121(include_top=True, weights = 'imagenet')\n",
    "cnn_model = Model(inputs=cnn_base_model.input, outputs=cnn_base_model.layers[-2].output)\n",
    "\n",
    "# Image feature input\n",
    "image_features_input = Input(shape=(1, 1024))  # Adjust the shape based on CNN output\n",
    "image_features = Dense(512, activation='relu')(image_features_input)\n",
    "image_features_reshaped = Reshape((1, 512), input_shape=(512,))(image_features)\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224, 3))  # Resize for InceptionV3\n",
    "    img = img_to_array(img)\n",
    "    img = tf.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    img = tf.keras.applications.densenet.preprocess_input(img)  # Preprocessing\n",
    "    features = cnn_model.predict(img, verbose=0)\n",
    "#     img = img_to_array(img)\n",
    "#     img = img/255.\n",
    "#     img = np.expand_dims(img,axis=0)\n",
    "    feature = cnn_model.predict(img, verbose=0)\n",
    "    feature = tf.reshape(feature, (1, -1))  # Flatten to (1, -1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b6475c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption input and embedding layer\n",
    "caption_input = Input(shape=(MAX_LENGTH,))\n",
    "caption_embedding = Embedding(VOCAB_SIZE, 256)(caption_input)\n",
    "caption_lstm = LSTM(512, return_sequences=True)(caption_embedding)\n",
    "\n",
    "# Combine CNN and LSTM outputs\n",
    "combined = add([image_features_reshaped, caption_lstm])\n",
    "output = LSTM(512)(combined)\n",
    "output = Dense(VOCAB_SIZE, activation='softmax')(output)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[image_features_input, caption_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f102d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, captions, batch_size=32):\n",
    "    num_samples = len(images)\n",
    "    while True:\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_images, batch_captions, output_captions = [], [], []\n",
    "            for j in range(batch_size):\n",
    "                if i+j >= num_samples:\n",
    "                    break\n",
    "                image, caption = images[i+j], captions[i+j]\n",
    "                features = FEATURES[image]\n",
    "                seq = vectorizer(caption)\n",
    "                batch_images.extend([features for s in range(1, len(seq))])\n",
    "                for k in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:k], seq[k]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=MAX_LENGTH)[0]\n",
    "                    out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vectorizer.vocabulary_size())[0]\n",
    "                    \n",
    "                    batch_captions.append(in_seq)\n",
    "                    output_captions.append(out_seq)\n",
    "            batch_captions, output_captions = tf.convert_to_tensor(np.array(batch_captions)), tf.convert_to_tensor(np.array(output_captions))\n",
    "            batch_images = tf.convert_to_tensor(np.array(batch_images))\n",
    "            \n",
    "            yield (batch_images, batch_captions), output_captions  # Input is a tuple, output is the target caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5be2fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features have been loaded\n"
     ]
    }
   ],
   "source": [
    "with open('features_densenet121.pkl', 'rb') as f:\n",
    "    FEATURES = pickle.load(f)\n",
    "    print('Image features have been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbe7a48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "234/234 [==============================] - 1247s 5s/step - loss: 4.3184 - val_loss: 3.4836\n",
      "Epoch 2/2\n",
      "234/234 [==============================] - 1243s 5s/step - loss: 3.1730 - val_loss: 3.0147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19433502890>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "BATCH_SIZE = 128\n",
    "train_generator = data_generator(train_imgs, train_captions, batch_size=BATCH_SIZE)\n",
    "val_generator = data_generator(val_imgs, val_captions, batch_size=BATCH_SIZE)\n",
    "model.fit(train_generator, batch_size=BATCH_SIZE, steps_per_epoch=len(train_imgs) // BATCH_SIZE, \n",
    "          epochs=2, validation_data=val_generator, validation_steps=len(val_imgs) // BATCH_SIZE, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b75e55f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 118s 760ms/step - loss: 2.9935\n",
      "Test Loss: 2.993499994277954\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "test_generator = data_generator(test_imgs, test_captions, batch_size=BATCH_SIZE)\n",
    "loss = model.evaluate(test_generator, steps=len(test_imgs) // BATCH_SIZE, verbose=1)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d5c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(img):\n",
    "    \n",
    "#     in_text = \"startseq\"\n",
    "    feat = extract_image_features(img)\n",
    "    for i in range(MAX_LENGTH):\n",
    "        sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
    "        sequence = pad_sequences([sequence],maxlen=MAX_LENGTH,padding='post')\n",
    "        final_caption = set()\n",
    "        ypred = model.predict([photo, sequence])\n",
    "        ypred = ypred.argmax() \n",
    "        word = idx_to_word[ypred]\n",
    "        in_text = ' ' + word\n",
    "        final_caption.add(in_text)\n",
    "        \n",
    "        print(in_text)\n",
    "        \n",
    "#         if word == \"endseq\":\n",
    "#             break\n",
    "    \n",
    "#         final_caption = in_text.split()[1:-1]\n",
    "        st = ' '.join(final_caption)\n",
    "        final_caption.clear()\n",
    "    \n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21792a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img, vectorizer):\n",
    "    feat = extract_image_features(img)\n",
    "    \n",
    "    caption = '<start>'\n",
    "    for i in range(MAX_LENGTH):\n",
    "        seq = vectorizer([caption])[0]\n",
    "        seq = pad_sequences([seq], maxlen=MAX_LENGTH)\n",
    "        \n",
    "        y_pred = cnn_model.predict([image_features, seq], verbose=0)\n",
    "        y_pred = np.argmax(y_pred)  # Get the index of the highest probability word\n",
    "        \n",
    "        # Map index to word\n",
    "        word = vectorizer.get_vocabulary()[y_pred]\n",
    "        caption.append(word)\n",
    "\n",
    "    return ' '.join(caption)  # Skip <start> and <end> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ecee7881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 1, 1024), (None, 10)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a9c4bdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a hiker ascends a snowy hill .'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_captions[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5d982b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<UNK> man         '"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = extract_image_features(test_imgs[100])\n",
    "feat = tf.convert_to_tensor(feat)\n",
    "feat =  tf.expand_dims(feat, axis=0)\n",
    "caption = '<UNK>'\n",
    "final_caption = set()\n",
    "for i in range(MAX_LENGTH):\n",
    "    seq = vectorizer([caption])[0]\n",
    "    seq = pad_sequences([seq], maxlen=MAX_LENGTH)[0]\n",
    "    in_seq = tf.convert_to_tensor(seq)\n",
    "    in_seq = tf.expand_dims(in_seq, axis=0)\n",
    "    y_pred = model.predict([feat, in_seq])\n",
    "    y_pred = np.argmax(y_pred)  # Get the index of the highest probability word\n",
    "\n",
    "    word = vectorizer.get_vocabulary()[y_pred]\n",
    "    caption = caption + ' ' + word\n",
    "    final_caption.add(word)\n",
    "\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7253edc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 2, 3, 4, 0, 0, 0, 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer('UNK a . in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9cfacb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['<start>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12907b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "64875523",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_densenet121.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433866b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
