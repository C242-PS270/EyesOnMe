{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43bdfa-e5e8-4560-a276-3b360f1e85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from textwrap import wrap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb29da8-b0fd-4ad8-bf0a-0d24a8d612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'flickr30k/Images'\n",
    "caption_path = 'flickr30k/captions.txt'\n",
    "\n",
    "data = pd.read_csv(caption_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19bf32-51c4-4711-a181-7fbd3631c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImage(path, img_size=224):\n",
    "    img = tf.keras.preprocessing.image.load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img / 255.\n",
    "    return img\n",
    "\n",
    "def display_images(temp_df):\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n += 1\n",
    "        plt.subplot(5, 5, n)\n",
    "        plt.subplots_adjust(hspace=0.7, wspace=0.3)\n",
    "        image = readImage(f\"C:/Users/muham/Documents/COOLYEAH/Semester 7/Bangkit/Capstone/EyesOnMe-ML/flickr30k/Images/{temp_df.image[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "display_images(data.sample(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af954f-9c96-466d-b842-4adcb4366e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    data['caption'] = data['caption'].astype(str)\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n",
    "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) > 1]))\n",
    "    data['caption'] = \"<start> \" + data['caption'] + \" <end>\"\n",
    "    return data\n",
    "\n",
    "data = text_preprocessing(data)\n",
    "captions = data['caption'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502081c-85d5-4b2b-8883-6ba11923cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "vectorizer.adapt(captions)\n",
    "\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "VOCAB_SIZE = len(vocabulary) + 1  # +1 for any special token, if needed\n",
    "MAX_LENGTH = max(len(caption.split()) for caption in captions)  # Same as before\n",
    "\n",
    "sequences = vectorizer(captions).numpy()\n",
    "padded_sequences = tf.keras.utils.pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "images = data['image'].unique().tolist()\n",
    "nimages = len(images)\n",
    "\n",
    "train_split = round(0.7 * nimages)\n",
    "val_split = round(0.85 * nimages)\n",
    "\n",
    "train_images = images[:train_split]\n",
    "val_images = images[train_split:val_split]\n",
    "test_images = images[val_split:]\n",
    "\n",
    "train_data = data[data['image'].isin(train_images)].reset_index(drop=True)\n",
    "val_data = data[data['image'].isin(val_images)].reset_index(drop=True)\n",
    "test_data = data[data['image'].isin(test_images)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560bba7-1781-4972-bad0-cb20fc7fdfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_data['image'].tolist()\n",
    "val_imgs = val_data['image'].tolist()\n",
    "test_imgs = test_data['image'].tolist()\n",
    "\n",
    "train_captions = train_data['caption'].tolist()\n",
    "val_captions = val_data['caption'].tolist()\n",
    "test_captions = test_data['caption'].tolist()\n",
    "\n",
    "def get_full_path(imgs):\n",
    "    return [f\"{image_path}/{img}\" for img in imgs]\n",
    "\n",
    "train_imgs = get_full_path(train_imgs)\n",
    "val_imgs = get_full_path(val_imgs)\n",
    "test_imgs = get_full_path(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235abbf-7647-40dc-afa8-734bc1e627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization( \n",
    "    standardize=None,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    ragged=True\n",
    ")\n",
    "vectorizer.adapt(train_captions)\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "\n",
    "print(len(vocabulary), vocabulary[:5], vectorizer(train_captions[0]), train_captions[0], end='\\n\\n')\n",
    "\n",
    "ragged = vectorizer(train_captions)\n",
    "padded = tf.keras.utils.pad_sequences(sequences=ragged.to_list(), padding='post', maxlen=MAX_LENGTH)\n",
    "\n",
    "ragged[0], padded[0], train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e712ff-a7e1-47df-ba1e-8a5e256358ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.DenseNet201()\n",
    "fe = tf.keras.models.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "img_size = 224\n",
    "imgpaths = np.unique(train_imgs + val_imgs + test_imgs)\n",
    "features = {}\n",
    "for imgpath in tqdm(imgpaths):\n",
    "    img = tf.keras.preprocessing.image.load_img(imgpath,target_size=(img_size,img_size))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img/255.\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    feature = fe.predict(img, verbose=0)\n",
    "    features[imgpath] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae857a-47ac-4bc2-8a1f-f93ead29c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('features.pkl', 'wb') as f:\n",
    "#     pickle.dump(features, f)\n",
    "\n",
    "# with open('features.pkl', 'rb') as f:\n",
    "#     features = pickle.load(f)\n",
    "features = {os.path.basename(path): value for path, value in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434600e-f6c9-4cc8-b859-0371e3de8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.keras.layers.Input(shape=(1, 1920))\n",
    "input2 = tf.keras.layers.Input(shape=(MAX_LENGTH,))\n",
    "\n",
    "img_features = tf.keras.layers.Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = tf.keras.layers.Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = tf.keras.layers.Embedding(VOCAB_SIZE, 256, mask_zero=False)(input2)\n",
    "merged = tf.keras.layers.concatenate([img_features_reshaped, sentence_features],axis=1)\n",
    "sentence_features = tf.keras.layers.LSTM(256)(merged)\n",
    "x = tf.keras.layers.Dropout(0.5)(sentence_features)\n",
    "x = tf.keras.layers.add([x, img_features])\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "output = tf.keras.layers.Dense(VOCAB_SIZE, activation='softmax')(x)\n",
    "\n",
    "caption_model = tf.keras.models.Model(inputs=[input1,input2], outputs=output)\n",
    "caption_model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac3390-f399-453b-af32-cffcdd320d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, X_col, y_col, features, vectorizer, vocab_size, max_length, batch_size, shuffle=True):\n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.features = features\n",
    "        self.vectorizer = vectorizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "        self.indices = np.arange(self.n)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        batch_indices = self.indices[start:end]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "\n",
    "        return self.__get_data(batch_df)\n",
    "\n",
    "    def __get_data(self, batch_df):\n",
    "        X1, X2, y = [], [], []\n",
    "    \n",
    "        images = batch_df[self.X_col].tolist()\n",
    "    \n",
    "        for image in images:\n",
    "            feature = self.features[image][0]\n",
    "            captions = batch_df.loc[batch_df[self.X_col] == image, self.y_col].tolist()\n",
    "    \n",
    "            for caption in captions:\n",
    "                seq = self.vectorizer(caption).numpy()\n",
    "    \n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = tf.keras.utils.pad_sequences([in_seq], maxlen=self.max_length, padding=\"pre\")[0]\n",
    "                    out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "    \n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "    \n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "        X1 = np.expand_dims(X1, axis=1)\n",
    "    \n",
    "        return (X1, X2), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a9281-e097-4ef7-bc16-b6023d4c8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(\n",
    "    df=train_data,\n",
    "    X_col='image',\n",
    "    y_col='caption',\n",
    "    features=features,\n",
    "    vectorizer=vectorizer,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = CustomDataGenerator(\n",
    "    df=test_data,\n",
    "    X_col='image',\n",
    "    y_col='caption',\n",
    "    features=features,\n",
    "    vectorizer=vectorizer,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0cad0-0f08-48ab-b6b3-9ac9ae310e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'image_captioning_model.keras'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_name,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                mode=\"min\",\n",
    "                                                save_best_only = True,\n",
    "                                                verbose=1)\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                               patience=3, \n",
    "                                                               verbose=1, \n",
    "                                                               factor=0.2, \n",
    "                                                               min_lr=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179b8ac-0594-4767-ac77-48e4b263239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = caption_model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75b903-125d-4184-84ff-b19b157a11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a66240-344f-412c-8407-ea8f7fcb7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_caption(model, image, vectorizer, max_length, features):\n",
    "#     feature = features[image]\n",
    "#     feature = tf.convert_to_tensor(feature)\n",
    "#     feature = tf.expand_dims(feature, axis=0)\n",
    "#     in_text = \"<start>\"\n",
    "#     for i in range(max_length):\n",
    "#         sequence = vectorizer([in_text])[0]\n",
    "#         sequence = tf.keras.utils.pad_sequences([sequence], max_length)\n",
    "        \n",
    "#         y_pred = model.predict((feature, sequence), verbose=0)\n",
    "#         y_pred = np.argmax(y_pred)\n",
    "        \n",
    "#         word = vectorizer.get_vocabulary()[y_pred]\n",
    "        \n",
    "#         if word is None:\n",
    "#             break\n",
    "            \n",
    "#         in_text += \" \" + word\n",
    "        \n",
    "#         if word == '<end>':            \n",
    "#             break\n",
    "            \n",
    "#     return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca462142-0ea9-49ed-a9cc-59d659fc3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = train_imgs[150]\n",
    "# img = tf.keras.preprocessing.image.load_img(path, color_mode='rgb',target_size=(244,244))\n",
    "# img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "# img /= 255\n",
    "# plt.imshow(img)\n",
    "# predict_caption(caption_model, path, vectorizer, MAX_LENGTH, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
